---
title: 'Milestone Report: Exploratory Data Analisys'
author: "Igor Goltsov <riversy@gmail.com>"
date: "June 11, 2016"
output: html_document
---

We will use a huge amount of data that provided by [HC Corpora](http://www.corpora.heliohost.org/). It will be used to train our app to predict next word in the text that user types. The data was collected by **HC Corpora** and recompiled for **Coursera** students by **Swiftkey** team. This package contains sets of News, Blog Posts and Tweets with 4 languages - English, Deutch, Finnish and Russian. I will use English texts for our learning purposes. 

Before we start let's ensure we have the data and load it. I will use speacially prepared utils to load it into varaibles with names *twitter.lines*, *blogs.lines*  and *news.lines*.

```{r results='hide', warning=FALSE, message=FALSE}
source('utils/requirements.R')
source('utils/data.R')
prepare_data()
```

Funsction *prepare_data()* will ensure that **Coursera-SwiftKey.zip** is downloaded and proper that necessery files are unzipped into *data* folder. 

## Examine files 

I will check the line qty and it's statistical properties for every file. I will also try to find more interesting facts about the data. 

Let's start from file sizes and the qty if line for every file. 

```{r, cache=TRUE, warning=FALSE}
get_file_by_name = function(name){
    paste(c('en_US', name, 'txt'), collapse = ".")
}

get_file_path_by_name = function(name){
    file_name = get_file_by_name(name)
    file_path = paste(c(data_folder_name, file_name), collapse = "/")    
}

get_data = function(name){
    file_path = get_file_path_by_name(name)
    scan(file_path, what = "character", sep = "\n")
}

# Sample size is 1%
sample_size = 0.01

twitter.lines <- get_data('twitter')
twitter.lines.lengths = nchar(twitter.lines)
twitter.lines.summary = summary(twitter.lines.lengths)
twitter.lines.sample = twitter.lines[
    sample(
        1:length(twitter.lines), 
        round(length(twitter.lines) * sample_size)
    )
]
rm(twitter.lines)

blogs.lines <- get_data('blogs')
blogs.lines.lengths = nchar(blogs.lines)
blogs.lines.summary = summary(blogs.lines.lengths)
blogs.lines.sample = blogs.lines[
    sample(
        1:length(blogs.lines), 
        round(length(blogs.lines) * sample_size)
    )
]
rm(blogs.lines)

news.lines <- get_data('news')
news.lines.lengths = nchar(news.lines)
news.lines.summary = summary(news.lines.lengths)
news.lines.sample = news.lines[
    sample(
        1:length(news.lines), 
        round(length(news.lines) * sample_size)
    )
]
rm(news.lines)

get_file_size_in_mb = function(name){
    file_path = get_file_path_by_name(name)
    file_info = file.info(file_path)
    file_info$size / 1024.0 / 1024.0
}

get_line_qty = function(variable){
    length(variable)
}

row.names = c('Tweets', 'Blog Posts', 'News Records')
row.file_names = c(get_file_by_name('twitter'), get_file_by_name('blogs'), get_file_by_name('news'))
row.file_sizes = c(get_file_size_in_mb('twitter'), get_file_size_in_mb('blogs'), get_file_size_in_mb('news'))
row.line_qtys = c(
    get_line_qty(twitter.lines.lengths), 
    get_line_qty(blogs.lines.lengths), 
    get_line_qty(news.lines.lengths)
)

row.min_lines = c(twitter.lines.summary[1], blogs.lines.summary[1], news.lines.summary[1])
row.max_lines = c(twitter.lines.summary[6], blogs.lines.summary[6], news.lines.summary[6])
row.mean_lines = c(twitter.lines.summary[4], blogs.lines.summary[4], news.lines.summary[4])
row.median_lines = c(twitter.lines.summary[3], blogs.lines.summary[3], news.lines.summary[3])

summary.table = data.frame(
    row.names, 
    row.file_names, 
    row.file_sizes, 
    row.line_qtys,
    row.min_lines,
    row.max_lines,
    row.mean_lines,
    row.median_lines
)

colnames(summary.table) = c(
    "Data Source", 
    "File Name",
    "File Size, Mb",
    "Lines in the file",
    "Min Line Lenght",
    "Max Line Lenght",
    "Avg Line Lenght",
    "Median of Line Lenght"
)
```

We've prepared data. So let's print it. 

```{r}
pander(summary.table, justify = 'left')
```

To understand the dencity of the lenghs for each of data source let's build violin plot. It will show us the real picture about length of lines in the corpus. 

```{r cache=TRUE}
twitter.df = data.frame(
    twitter.lines.lengths, 
    rep("Twitter", length(twitter.lines.lengths))
)
colnames(twitter.df) = c('length', 'source') 

blogs.df = data.frame(
    blogs.lines.lengths, 
    rep("Blogs", length(blogs.lines.lengths))
)
colnames(blogs.df) = c('length', 'source') 

news.df = data.frame(
    news.lines.lengths, 
    rep("News", length(news.lines.lengths))
)
colnames(news.df) = c('length', 'source') 

combined.df = rbind(twitter.df, blogs.df, news.df)
```

We can see by **Median of Line Length** that it's mostly below *200* and the max length of line is `r max(row.max_lines)`. So I will limit the **lengths** axis to make it more clear. And will add median mark 

```{r warning=FALSE, out.width=800}
ggplot(combined.df, aes(x=factor(source), y=length)) + 
    geom_violin() +
    geom_boxplot(width=.1, fill="black", outlier.colour=NA) +
    stat_summary(fun.y=median, geom="point", fill="white", shape=21, size=2.5) +
    ylim(0, 1000)
```

Wat can we resume? That the **Tweets**, **Blog Posts** and **News Lines** has mostly same size. But **Blog Posts** and **News** has much more higther maximum length of lines.

## Keyword Dencity Analysis

To realize the content of the gived sources I'd like clean it up and tokenize it with n-grams. But as soon as the original data source is huge I need to prepare the sample data set that is smaller. And it's easier to work with. 

### Samplpling the Data

I will prepare the sample that cantains 10% of all provided data. For that let's use 10% samples I've prepared before. 

```{r}
sample.lines  = c(twitter.lines.sample, blogs.lines.sample, news.lines.sample)
rm(twitter.lines.sample)
rm(blogs.lines.sample)
rm(news.lines.sample)
```

Now I have sample data set that contains `r length(sample.lines)` lines with representative data. We can easly process it but in the other hand we can make some suggestions on the basis of that data. 

### Clean Up Sample Data Set

On this step I will create new *Corpus* of the **tm** package and remove from text stop words, numbers and punctuation. I also would like to remove profanity words from all texts to prevent my work from being confusing. I will use following list as profanity dictionary - [https://gist.github.com/ryanlewis/a37739d710ccdb4b406d](https://gist.github.com/ryanlewis/a37739d710ccdb4b406d).

```{r cache=TRUE}
# Create Corpus of Sample
sample.corpus = Corpus(VectorSource(sample.lines))

# Remove variable with Sample Vector to reduce
# memory consumption.
rm(sample.lines)

# I've examined control characters in the body of some texts.
# And I'd like to make it clean from these characters.
convert_encoding = function(x){
    iconv(x, to="UTF-8", sub="byte")
}

sample.corpus = tm_map(
    sample.corpus, 
    content_transformer(convert_encoding), 
    mc.cores = 4
)

# Convert to lower case
sample.corpus = tm_map(
    sample.corpus, 
    content_transformer(tolower), 
    lazy = TRUE
)

# Remove punctuation
sample.corpus = tm_map(sample.corpus, content_transformer(removePunctuation))

# Remove numbers
sample.corpus = tm_map(sample.corpus, content_transformer(removeNumbers))

# Remove URLs
remove_url_callback = function(x){
    gsub("http[[:alnum:]]*", "", x) 
}
sample.corpus = tm_map(sample.corpus, content_transformer(remove_url_callback))

# Remove stop words
sample.corpus = tm_map(sample.corpus, content_transformer(removeWords), stopwords("english"))

# Remove profanity words
profanity_words = scan("dictionary/profanity.txt", what = "character", sep = "\n")
sample.corpus = tm_map(sample.corpus, content_transformer(removeWords), profanity_words)

# Stem document
sample.corpus = tm_map(sample.corpus, content_transformer(stemDocument))

# Strip white spaces 
sample.corpus = tm_map(sample.corpus, content_transformer(stripWhitespace))
```

### Tockenize Cleaned Sample Corpus

Ok, the data is clear and I can tokenize it using 1,2,3 and 4-grams. After that I will define more recent used words and build wordcloud for these sets of words. 

```{r cache=TRUE}
# Tune CPU cores usage 
options(mc.cores=4)

sample.corpus.df = data.frame(
    text=unlist(sapply(sample.corpus,`[`, "content")), 
    stringsAsFactors = FALSE
)

# Free memory
rm(sample.corpus)

get_ngrams <- function(corpus, ngram_count = 1, limit = FALSE){

    ngrams <- NGramTokenizer(
        corpus, 
        Weka_control(
            min = ngram_count, 
            max = ngram_count, 
            delimiters = " \\r\\n\\t.,;:\"()?!"
        )
    )
    ngrams <- data.frame(table(ngrams))
    if (limit){
        ngrams <- ngrams[
            order(
                ngrams$Freq, 
                decreasing = TRUE
            ),
        ][1:limit, ]
    } else {
        ngrams <- ngrams[
            order(
                ngrams$Freq, 
                decreasing = TRUE
            ),
        ]
    }
    colnames(ngrams) <- c("String","Count")
    ngrams
}
 
unigrams <- get_ngrams(sample.corpus.df, 1, 50)
bigrams <- get_ngrams(sample.corpus.df, 2, 50)
trigrams <- get_ngrams(sample.corpus.df, 3, 50)
```

After we've prepared *n-grams* we can plot it on wordcloud. I'd like to use wordcloud because it helps to understand how word dencity looks like. 

#### Top 50 Unigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = unigrams$String, 
  freq = unigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

#### Top 50 Bigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = bigrams$String, 
  freq = bigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

#### Top 50 Trigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = trigrams$String, 
  freq = trigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

Looks great! We can see top 50 of *n-grams* which used in corpus. So we can realize what *n-grams* most recently used in the texts. Sure, we discovered sample only. But in statistical matter the sample completely represents it's source. 

### Conclusions

+ We can see that the most of all texts in data sets are not so long. Mostly it's length is from 0 to 250 characters. And it contains not many sentences. So we can reduce our work and use our clean up model for prediction purposes. 
+ The most used words are really looks like widely used. So we have chance to build really working prediction model if we will use *n-grams* as the base for our prediction.
+ The data we work with is a bit huge and it consume a lot of memory. First of all I will try to use corpus DB on cleanup stage. And in the other hand I will cache the results of calculation into RDS files. That will help me to deploy the results of works to publicity. 
+ I've found that using of multiprocessing is great if word with huge data. I've used *mc.cores* option with *tm_map()* and will try to find ways to use similar techincue on prediction stage. As soon as really increase the performance of the alghorithm.

Also there are some conclustions that can help to make prediction model better. 

+ There are some abbreviations we should convert into full form on cleaning up stage. For example *I'm* should be converted into *I am* etc. We will have more consistant data due to this. 
+ I think we shouldn't remove stop-words according to purposes of this work, because we build prediction model which helps humans to type text. And the stopwords is the important part of the natural language. At least I will try to test prediction models with and without stopwords to test which will works better. 

P.S. Then I started the *NGramTokenizer* on 10% sample I've got odd memory linked with *Out Of Java Memory*. The cutting vector into pieces helped me but I'm looking to *quanteda* package to use it in my later works. 















